{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MarianMTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/tl2020/.local/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a cat\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[58100,   103,    29,    50, 23859,     3,     0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Das ist eine Katze.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   60,    19,    14, 12064,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</s>': 0,\n",
       " '<unk>': 1,\n",
       " ',': 2,\n",
       " '.': 3,\n",
       " '▁the': 4,\n",
       " '▁in': 5,\n",
       " 's': 6,\n",
       " '▁of': 7,\n",
       " '▁and': 8,\n",
       " '▁der': 9,\n",
       " '▁und': 10,\n",
       " '▁die': 11,\n",
       " '▁to': 12,\n",
       " '-': 13,\n",
       " '▁a': 14,\n",
       " 'en': 15,\n",
       " ':': 16,\n",
       " '▁': 17,\n",
       " 'e': 18,\n",
       " '▁is': 19,\n",
       " '▁(': 20,\n",
       " '▁von': 21,\n",
       " \"'\": 22,\n",
       " '▁for': 23,\n",
       " '▁zu': 24,\n",
       " '▁den': 25,\n",
       " '▁-': 26,\n",
       " ')': 27,\n",
       " '▁für': 28,\n",
       " '▁ist': 29,\n",
       " '▁mit': 30,\n",
       " '?': 31,\n",
       " '▁on': 32,\n",
       " '▁with': 33,\n",
       " 'n': 34,\n",
       " '▁that': 35,\n",
       " '▁The': 36,\n",
       " '▁auf': 37,\n",
       " '▁I': 38,\n",
       " '▁an': 39,\n",
       " '▁des': 40,\n",
       " '▁you': 41,\n",
       " '▁Sie': 42,\n",
       " '▁be': 43,\n",
       " '▁das': 44,\n",
       " 'er': 45,\n",
       " 't': 46,\n",
       " '▁\"': 47,\n",
       " '▁are': 48,\n",
       " '▁im': 49,\n",
       " '▁eine': 50,\n",
       " '▁nicht': 51,\n",
       " '▁as': 52,\n",
       " '▁ein': 53,\n",
       " '▁by': 54,\n",
       " '▁Die': 55,\n",
       " '▁it': 56,\n",
       " '▁dem': 57,\n",
       " '▁was': 58,\n",
       " '▁from': 59,\n",
       " '▁this': 60,\n",
       " '▁werden': 61,\n",
       " '▁or': 62,\n",
       " '▁sich': 63,\n",
       " '▁In': 64,\n",
       " '▁es': 65,\n",
       " '▁not': 66,\n",
       " '▁at': 67,\n",
       " '!': 68,\n",
       " '▁oder': 69,\n",
       " '▁sind': 70,\n",
       " '\"': 71,\n",
       " '▁have': 72,\n",
       " '▁will': 73,\n",
       " '▁aus': 74,\n",
       " '/': 75,\n",
       " '▁sie': 76,\n",
       " ';': 77,\n",
       " '▁als': 78,\n",
       " 'ing': 79,\n",
       " '▁wird': 80,\n",
       " '▁über': 81,\n",
       " 'd': 82,\n",
       " '▁your': 83,\n",
       " '▁ich': 84,\n",
       " '▁can': 85,\n",
       " '▁which': 86,\n",
       " '▁dass': 87,\n",
       " '▁so': 88,\n",
       " '▁auch': 89,\n",
       " 'm': 90,\n",
       " '▁um': 91,\n",
       " '▁all': 92,\n",
       " '▁A': 93,\n",
       " '▁einer': 94,\n",
       " '▁we': 95,\n",
       " '▁nach': 96,\n",
       " 'es': 97,\n",
       " '...': 98,\n",
       " '▁has': 99,\n",
       " '▁wir': 100,\n",
       " '▁zur': 101,\n",
       " '▁bei': 102,\n",
       " '▁Das': 103,\n",
       " 'r': 104,\n",
       " '▁Ich': 105,\n",
       " '▁einen': 106,\n",
       " '▁wie': 107,\n",
       " 'ed': 108,\n",
       " '▁hat': 109,\n",
       " '▁er': 110,\n",
       " 'a': 111,\n",
       " '▁–': 112,\n",
       " '▁1': 113,\n",
       " '▁haben': 114,\n",
       " '▁also': 115,\n",
       " '▁2': 116,\n",
       " '▁einem': 117,\n",
       " 'in': 118,\n",
       " '▁Der': 119,\n",
       " '▁durch': 120,\n",
       " '▁am': 121,\n",
       " ').': 122,\n",
       " '▁zum': 123,\n",
       " '▁S': 124,\n",
       " '▁our': 125,\n",
       " '▁more': 126,\n",
       " '▁können': 127,\n",
       " '▁one': 128,\n",
       " '▁3': 129,\n",
       " '▁their': 130,\n",
       " '▁It': 131,\n",
       " 'y': 132,\n",
       " '▁war': 133,\n",
       " '▁kann': 134,\n",
       " 're': 135,\n",
       " '▁its': 136,\n",
       " '▁he': 137,\n",
       " '▁wurde': 138,\n",
       " '▁nur': 139,\n",
       " '▁This': 140,\n",
       " '▁they': 141,\n",
       " 'S': 142,\n",
       " '▁du': 143,\n",
       " '▁but': 144,\n",
       " '▁vor': 145,\n",
       " '▁You': 146,\n",
       " '▁other': 147,\n",
       " '▁B': 148,\n",
       " '▁Es': 149,\n",
       " '▁up': 150,\n",
       " '▁European': 151,\n",
       " '▁been': 152,\n",
       " '▁about': 153,\n",
       " '▁his': 154,\n",
       " '▁E': 155,\n",
       " '▁do': 156,\n",
       " '▁We': 157,\n",
       " '▁uns': 158,\n",
       " '▁bis': 159,\n",
       " '▁time': 160,\n",
       " '▁out': 161,\n",
       " '▁vom': 162,\n",
       " '),': 163,\n",
       " '▁me': 164,\n",
       " '▁&': 165,\n",
       " 'o': 166,\n",
       " '▁sein': 167,\n",
       " '▁new': 168,\n",
       " '▁there': 169,\n",
       " '▁diese': 170,\n",
       " '▁dieser': 171,\n",
       " '▁were': 172,\n",
       " '▁5': 173,\n",
       " '▁noch': 174,\n",
       " '▁man': 175,\n",
       " '▁wenn': 176,\n",
       " '▁only': 177,\n",
       " 'de': 178,\n",
       " '▁aber': 179,\n",
       " 'ung': 180,\n",
       " 'ly': 181,\n",
       " '▁Wir': 182,\n",
       " '▁C': 183,\n",
       " '▁4': 184,\n",
       " '▁mehr': 185,\n",
       " 'C': 186,\n",
       " '▁my': 187,\n",
       " 'B': 188,\n",
       " '▁who': 189,\n",
       " '▁should': 190,\n",
       " '▁no': 191,\n",
       " '▁unter': 192,\n",
       " '▁—': 193,\n",
       " '▁daß': 194,\n",
       " '▁Commission': 195,\n",
       " '▁into': 196,\n",
       " 'i': 197,\n",
       " '▁„': 198,\n",
       " '▁EU': 199,\n",
       " '▁No': 200,\n",
       " '▁Er': 201,\n",
       " '▁L': 202,\n",
       " '▁Kommission': 203,\n",
       " '▁10': 204,\n",
       " '▁if': 205,\n",
       " '▁would': 206,\n",
       " '▁what': 207,\n",
       " '▁them': 208,\n",
       " '▁like': 209,\n",
       " '▁us': 210,\n",
       " 'A': 211,\n",
       " '▁alle': 212,\n",
       " 'al': 213,\n",
       " 'g': 214,\n",
       " '▁had': 215,\n",
       " 'h': 216,\n",
       " '2': 217,\n",
       " '▁shall': 218,\n",
       " '▁than': 219,\n",
       " '▁any': 220,\n",
       " '.\"': 221,\n",
       " '▁eines': 222,\n",
       " 'st': 223,\n",
       " '▁use': 224,\n",
       " 'D': 225,\n",
       " '▁first': 226,\n",
       " 'te': 227,\n",
       " '▁Und': 228,\n",
       " '▁information': 229,\n",
       " '▁de': 230,\n",
       " '▁He': 231,\n",
       " '▁when': 232,\n",
       " '▁T': 233,\n",
       " '▁6': 234,\n",
       " 'b': 235,\n",
       " '▁And': 236,\n",
       " '▁these': 237,\n",
       " '▁people': 238,\n",
       " 'z': 239,\n",
       " '▁Artikel': 240,\n",
       " '▁Union': 241,\n",
       " '▁P': 242,\n",
       " '▁such': 243,\n",
       " '▁hier': 244,\n",
       " '▁ab': 245,\n",
       " '▁Ein': 246,\n",
       " '▁sowie': 247,\n",
       " '▁So': 248,\n",
       " '▁her': 249,\n",
       " 'k': 250,\n",
       " '▁well': 251,\n",
       " '▁may': 252,\n",
       " '▁know': 253,\n",
       " '▁two': 254,\n",
       " \"▁'\": 255,\n",
       " '▁very': 256,\n",
       " '▁between': 257,\n",
       " '▁ver': 258,\n",
       " '▁here': 259,\n",
       " '▁mich': 260,\n",
       " '▁sehr': 261,\n",
       " '▁ihre': 262,\n",
       " '▁D': 263,\n",
       " '▁just': 264,\n",
       " 'c': 265,\n",
       " '▁over': 266,\n",
       " '▁M': 267,\n",
       " '▁18': 268,\n",
       " '▁work': 269,\n",
       " '▁G': 270,\n",
       " '▁years': 271,\n",
       " '▁K': 272,\n",
       " '▁zwischen': 273,\n",
       " '▁Member': 274,\n",
       " '▁diesem': 275,\n",
       " '▁anderen': 276,\n",
       " '▁keine': 277,\n",
       " '▁mir': 278,\n",
       " 'den': 279,\n",
       " '▁now': 280,\n",
       " '▁Im': 281,\n",
       " '▁An': 282,\n",
       " '▁States': 283,\n",
       " '▁ihr': 284,\n",
       " '▁m': 285,\n",
       " '▁some': 286,\n",
       " 'an': 287,\n",
       " 'th': 288,\n",
       " '▁Ihre': 289,\n",
       " 'll': 290,\n",
       " '▁20': 291,\n",
       " '▁For': 292,\n",
       " '▁/': 293,\n",
       " '▁F': 294,\n",
       " '▁If': 295,\n",
       " '▁Zeit': 296,\n",
       " '▁gibt': 297,\n",
       " '▁wurden': 298,\n",
       " '▁where': 299,\n",
       " '▁7': 300,\n",
       " '▁must': 301,\n",
       " '▁Wenn': 302,\n",
       " '▁him': 303,\n",
       " 'F': 304,\n",
       " '3': 305,\n",
       " '▁most': 306,\n",
       " '▁What': 307,\n",
       " '▁Hotel': 308,\n",
       " '▁8': 309,\n",
       " '▁da': 310,\n",
       " '▁under': 311,\n",
       " '▁Be': 312,\n",
       " '▁Du': 313,\n",
       " 'ten': 314,\n",
       " '▁right': 315,\n",
       " 'or': 316,\n",
       " '▁Council': 317,\n",
       " '▁un': 318,\n",
       " '▁made': 319,\n",
       " '▁Europäischen': 320,\n",
       " 'E': 321,\n",
       " '▁e': 322,\n",
       " '▁Ver': 323,\n",
       " '▁dann': 324,\n",
       " '▁müssen': 325,\n",
       " 'K': 326,\n",
       " '\",': 327,\n",
       " '▁get': 328,\n",
       " '▁V': 329,\n",
       " '▁data': 330,\n",
       " '▁...': 331,\n",
       " '▁year': 332,\n",
       " '▁Diese': 333,\n",
       " '▁habe': 334,\n",
       " '▁15': 335,\n",
       " ']': 336,\n",
       " 'us': 337,\n",
       " '▁zwei': 338,\n",
       " '▁take': 339,\n",
       " '▁make': 340,\n",
       " '▁%': 341,\n",
       " 'H': 342,\n",
       " '▁per': 343,\n",
       " '▁12': 344,\n",
       " '▁Jahr': 345,\n",
       " '▁_': 346,\n",
       " '▁30': 347,\n",
       " 'l': 348,\n",
       " '▁after': 349,\n",
       " 'P': 350,\n",
       " 'f': 351,\n",
       " '▁Was': 352,\n",
       " '▁immer': 353,\n",
       " 'be': 354,\n",
       " '▁need': 355,\n",
       " '▁those': 356,\n",
       " '▁R': 357,\n",
       " 've': 358,\n",
       " '▁used': 359,\n",
       " '▁world': 360,\n",
       " '▁dieses': 361,\n",
       " '▁way': 362,\n",
       " '▁seine': 363,\n",
       " 'V': 364,\n",
       " '1': 365,\n",
       " '▁products': 366,\n",
       " '▁Community': 367,\n",
       " '▁system': 368,\n",
       " '▁Daten': 369,\n",
       " '▁Article': 370,\n",
       " '▁re': 371,\n",
       " '▁ge': 372,\n",
       " '▁Ihnen': 373,\n",
       " '▁many': 374,\n",
       " 'G': 375,\n",
       " '▁Mit': 376,\n",
       " 'ge': 377,\n",
       " '▁H': 378,\n",
       " '▁see': 379,\n",
       " 'to': 380,\n",
       " '▁Mr': 381,\n",
       " 'com': 382,\n",
       " 'T': 383,\n",
       " '▁then': 384,\n",
       " '▁[': 385,\n",
       " '▁through': 386,\n",
       " 'M': 387,\n",
       " '▁Auf': 388,\n",
       " '▁order': 389,\n",
       " '▁But': 390,\n",
       " '▁km': 391,\n",
       " '▁want': 392,\n",
       " 'on': 393,\n",
       " '▁N': 394,\n",
       " '▁9': 395,\n",
       " '▁Informationen': 396,\n",
       " '▁could': 397,\n",
       " 'W': 398,\n",
       " '▁All': 399,\n",
       " '\".': 400,\n",
       " '▁Welt': 401,\n",
       " '▁good': 402,\n",
       " 'x': 403,\n",
       " 'The': 404,\n",
       " '▁area': 405,\n",
       " '▁how': 406,\n",
       " '▁part': 407,\n",
       " '▁available': 408,\n",
       " '▁Menschen': 409,\n",
       " '▁andere': 410,\n",
       " '▁Unternehmen': 411,\n",
       " '▁even': 412,\n",
       " '▁don': 413,\n",
       " '▁Art': 414,\n",
       " '▁W': 415,\n",
       " '▁high': 416,\n",
       " '▁€': 417,\n",
       " 'em': 418,\n",
       " '▁countries': 419,\n",
       " '▁pro': 420,\n",
       " '▁day': 421,\n",
       " '▁back': 422,\n",
       " '▁Wie': 423,\n",
       " '▁Entwicklung': 424,\n",
       " '▁Regulation': 425,\n",
       " '▁same': 426,\n",
       " '▁wieder': 427,\n",
       " '▁market': 428,\n",
       " '▁To': 429,\n",
       " '▁go': 430,\n",
       " '▁Jahre': 431,\n",
       " '▁As': 432,\n",
       " '▁because': 433,\n",
       " '▁neue': 434,\n",
       " 'N': 435,\n",
       " '▁ihrer': 436,\n",
       " '▁before': 437,\n",
       " '▁number': 438,\n",
       " '▁within': 439,\n",
       " 'ation': 440,\n",
       " '▁There': 441,\n",
       " 'le': 442,\n",
       " '▁place': 443,\n",
       " 'I': 444,\n",
       " '▁Mitgliedstaaten': 445,\n",
       " '▁damit': 446,\n",
       " '▁muss': 447,\n",
       " '▁0': 448,\n",
       " '▁ihn': 449,\n",
       " '▁find': 450,\n",
       " '▁ohne': 451,\n",
       " '▁set': 452,\n",
       " '▁Europe': 453,\n",
       " '▁life': 454,\n",
       " 'ver': 455,\n",
       " '▁Verordnung': 456,\n",
       " '▁being': 457,\n",
       " '▁schon': 458,\n",
       " '▁following': 459,\n",
       " 'ia': 460,\n",
       " '▁free': 461,\n",
       " 'ers': 462,\n",
       " '▁down': 463,\n",
       " '▁p': 464,\n",
       " '▁Re': 465,\n",
       " '▁said': 466,\n",
       " '▁dich': 467,\n",
       " '▁Jahren': 468,\n",
       " '▁Ihr': 469,\n",
       " '▁O': 470,\n",
       " '▁unsere': 471,\n",
       " '▁:': 472,\n",
       " '▁she': 473,\n",
       " '▁God': 474,\n",
       " '(': 475,\n",
       " '▁New': 476,\n",
       " 'um': 477,\n",
       " '▁development': 478,\n",
       " '▁Stadt': 479,\n",
       " '▁different': 480,\n",
       " '▁gegen': 481,\n",
       " '▁Nr': 482,\n",
       " '▁finden': 483,\n",
       " '▁seiner': 484,\n",
       " '▁diesen': 485,\n",
       " 'et': 486,\n",
       " '▁That': 487,\n",
       " '▁online': 488,\n",
       " '▁1.': 489,\n",
       " '▁Für': 490,\n",
       " '▁They': 491,\n",
       " 'ch': 492,\n",
       " 'is': 493,\n",
       " '▁product': 494,\n",
       " '▁bin': 495,\n",
       " '▁On': 496,\n",
       " '4': 497,\n",
       " '▁Ab': 498,\n",
       " '▁waren': 499,\n",
       " 'the': 500,\n",
       " '▁important': 501,\n",
       " '▁Nach': 502,\n",
       " 'p': 503,\n",
       " '▁without': 504,\n",
       " '▁long': 505,\n",
       " '▁each': 506,\n",
       " '▁17': 507,\n",
       " '▁own': 508,\n",
       " '▁16': 509,\n",
       " '▁U': 510,\n",
       " '▁Gemeinschaft': 511,\n",
       " '▁11': 512,\n",
       " '▁hatte': 513,\n",
       " '▁.': 514,\n",
       " '▁sollte': 515,\n",
       " 'able': 516,\n",
       " '▁best': 517,\n",
       " '▁Ge': 518,\n",
       " '▁gut': 519,\n",
       " '▁support': 520,\n",
       " '▁possible': 521,\n",
       " '▁machen': 522,\n",
       " 'L': 523,\n",
       " 'man': 524,\n",
       " '▁Eine': 525,\n",
       " 'el': 526,\n",
       " '▁case': 527,\n",
       " 'at': 528,\n",
       " '▁Aber': 529,\n",
       " '▁still': 530,\n",
       " '▁service': 531,\n",
       " '▁services': 532,\n",
       " '▁neuen': 533,\n",
       " '▁three': 534,\n",
       " '▁does': 535,\n",
       " '▁14': 536,\n",
       " 'der': 537,\n",
       " '▁jetzt': 538,\n",
       " '▁Teil': 539,\n",
       " 'ic': 540,\n",
       " '▁selbst': 541,\n",
       " 'EC': 542,\n",
       " '▁Vor': 543,\n",
       " '▁did': 544,\n",
       " '▁Land': 545,\n",
       " '▁during': 546,\n",
       " '▁both': 547,\n",
       " '▁help': 548,\n",
       " '▁Aus': 549,\n",
       " '▁St': 550,\n",
       " '▁etwas': 551,\n",
       " 'ne': 552,\n",
       " '▁denen': 553,\n",
       " '▁25': 554,\n",
       " '▁Germany': 555,\n",
       " '▁Ja': 556,\n",
       " '▁während': 557,\n",
       " '▁come': 558,\n",
       " '▁last': 559,\n",
       " '▁since': 560,\n",
       " '▁public': 561,\n",
       " '▁ihren': 562,\n",
       " '▁wo': 563,\n",
       " '▁car': 564,\n",
       " 'na': 565,\n",
       " '▁ihm': 566,\n",
       " '▁drei': 567,\n",
       " '▁social': 568,\n",
       " '▁think': 569,\n",
       " 'it': 570,\n",
       " '▁Rahmen': 571,\n",
       " '▁Am': 572,\n",
       " '▁EUR': 573,\n",
       " '▁At': 574,\n",
       " '▁level': 575,\n",
       " '▁policy': 576,\n",
       " '▁100': 577,\n",
       " '▁La': 578,\n",
       " '▁»': 579,\n",
       " '▁Um': 580,\n",
       " '▁zurück': 581,\n",
       " '▁seit': 582,\n",
       " '▁jedoch': 583,\n",
       " '▁much': 584,\n",
       " '_': 585,\n",
       " '▁German': 586,\n",
       " '▁My': 587,\n",
       " '▁Dr': 588,\n",
       " '▁Z': 589,\n",
       " '▁Tag': 590,\n",
       " '▁Da': 591,\n",
       " '▁unserer': 592,\n",
       " '▁against': 593,\n",
       " '▁Bei': 594,\n",
       " '▁50': 595,\n",
       " '▁Maßnahmen': 596,\n",
       " '▁national': 597,\n",
       " '▁end': 598,\n",
       " '▁De': 599,\n",
       " '▁dir': 600,\n",
       " '▁bietet': 601,\n",
       " '▁Leben': 602,\n",
       " 'u': 603,\n",
       " '▁Über': 604,\n",
       " 'Z': 605,\n",
       " '▁sollten': 606,\n",
       " '▁ersten': 607,\n",
       " '▁Seite': 608,\n",
       " 'In': 609,\n",
       " '▁offers': 610,\n",
       " '▁bereits': 611,\n",
       " '▁13': 612,\n",
       " '▁international': 613,\n",
       " '▁great': 614,\n",
       " '▁measures': 615,\n",
       " '▁water': 616,\n",
       " '▁allen': 617,\n",
       " 'land': 618,\n",
       " '▁sondern': 619,\n",
       " 'EG': 620,\n",
       " 'ter': 621,\n",
       " '▁meine': 622,\n",
       " 'R': 623,\n",
       " 'O': 624,\n",
       " '▁Absatz': 625,\n",
       " '▁Parliament': 626,\n",
       " '▁viele': 627,\n",
       " '▁ob': 628,\n",
       " '▁Do': 629,\n",
       " '▁doch': 630,\n",
       " 'w': 631,\n",
       " '▁denn': 632,\n",
       " '▁Home': 633,\n",
       " '▁conditions': 634,\n",
       " '▁point': 635,\n",
       " '▁every': 636,\n",
       " 'ar': 637,\n",
       " '▁Service': 638,\n",
       " 'ab': 639,\n",
       " '▁24': 640,\n",
       " 'up': 641,\n",
       " '▁city': 642,\n",
       " '▁company': 643,\n",
       " '▁sehen': 644,\n",
       " '▁soll': 645,\n",
       " '▁already': 646,\n",
       " '▁production': 647,\n",
       " '▁around': 648,\n",
       " 'sch': 649,\n",
       " '▁How': 650,\n",
       " 'ist': 651,\n",
       " '▁geht': 652,\n",
       " '▁days': 653,\n",
       " '▁Kinder': 654,\n",
       " '▁Alle': 655,\n",
       " '▁file': 656,\n",
       " '▁particular': 657,\n",
       " 'ity': 658,\n",
       " '▁Deutschland': 659,\n",
       " 'ig': 660,\n",
       " '▁Preis': 661,\n",
       " '▁including': 662,\n",
       " '▁Internet': 663,\n",
       " '▁quality': 664,\n",
       " '▁viel': 665,\n",
       " '▁heute': 666,\n",
       " '▁alles': 667,\n",
       " '▁Berlin': 668,\n",
       " '▁Produkt': 669,\n",
       " 'co': 670,\n",
       " '▁name': 671,\n",
       " '▁Wasser': 672,\n",
       " '▁Europa': 673,\n",
       " '▁State': 674,\n",
       " '▁too': 675,\n",
       " '▁say': 676,\n",
       " 'se': 677,\n",
       " '▁ihnen': 678,\n",
       " '▁Arbeit': 679,\n",
       " '▁means': 680,\n",
       " '▁einfach': 681,\n",
       " '▁room': 682,\n",
       " '▁country': 683,\n",
       " '▁With': 684,\n",
       " 'im': 685,\n",
       " '▁business': 686,\n",
       " '▁zusammen': 687,\n",
       " '▁Rat': 688,\n",
       " '▁English': 689,\n",
       " '▁Herr': 690,\n",
       " '▁Man': 691,\n",
       " 'Die': 692,\n",
       " '▁*': 693,\n",
       " '▁J': 694,\n",
       " 'ie': 695,\n",
       " '▁again': 696,\n",
       " '▁small': 697,\n",
       " '▁19': 698,\n",
       " '▁economic': 699,\n",
       " '▁b': 700,\n",
       " '▁erhalten': 701,\n",
       " '▁ganz': 702,\n",
       " 'ion': 703,\n",
       " '▁view': 704,\n",
       " 'ur': 705,\n",
       " '▁research': 706,\n",
       " '▁Euro': 707,\n",
       " '▁weiter': 708,\n",
       " '▁give': 709,\n",
       " '▁President': 710,\n",
       " '▁d': 711,\n",
       " '▁period': 712,\n",
       " '▁process': 713,\n",
       " '▁report': 714,\n",
       " '▁seinen': 715,\n",
       " '▁beim': 716,\n",
       " 'ische': 717,\n",
       " '▁würde': 718,\n",
       " '▁United': 719,\n",
       " '▁liegt': 720,\n",
       " '▁X': 721,\n",
       " '▁wissen': 722,\n",
       " '▁found': 723,\n",
       " '5': 724,\n",
       " 'lich': 725,\n",
       " '▁Co': 726,\n",
       " '▁form': 727,\n",
       " 'ment': 728,\n",
       " '▁September': 729,\n",
       " 'ton': 730,\n",
       " 'ner': 731,\n",
       " '▁Bereich': 732,\n",
       " '▁+': 733,\n",
       " '▁children': 734,\n",
       " '▁further': 735,\n",
       " '▁g': 736,\n",
       " '▁November': 737,\n",
       " '▁Committee': 738,\n",
       " '▁price': 739,\n",
       " '▁open': 740,\n",
       " '▁Produkte': 741,\n",
       " '▁==': 742,\n",
       " '▁while': 743,\n",
       " '▁System': 744,\n",
       " 'sten': 745,\n",
       " '▁She': 746,\n",
       " '▁2015': 747,\n",
       " '▁21': 748,\n",
       " 'gen': 749,\n",
       " '▁Kunden': 750,\n",
       " '▁going': 751,\n",
       " '▁euch': 752,\n",
       " '▁got': 753,\n",
       " '▁energy': 754,\n",
       " '▁provided': 755,\n",
       " '▁million': 756,\n",
       " '▁April': 757,\n",
       " 'ry': 758,\n",
       " 'iert': 759,\n",
       " 'und': 760,\n",
       " '*': 761,\n",
       " '▁example': 762,\n",
       " '▁Richtlinie': 763,\n",
       " '▁tun': 764,\n",
       " '▁weil': 765,\n",
       " '▁Pro': 766,\n",
       " '▁Gott': 767,\n",
       " '▁Zu': 768,\n",
       " '▁times': 769,\n",
       " '▁able': 770,\n",
       " '▁working': 771,\n",
       " '▁40': 772,\n",
       " '▁special': 773,\n",
       " '▁When': 774,\n",
       " '▁Kontakt': 775,\n",
       " 'ern': 776,\n",
       " '▁Deutsch': 777,\n",
       " '▁Your': 778,\n",
       " '▁Fall': 779,\n",
       " '▁course': 780,\n",
       " 'ieren': 781,\n",
       " '▁Frage': 782,\n",
       " '▁Zimmer': 783,\n",
       " '▁•': 784,\n",
       " '▁gemäß': 785,\n",
       " '▁application': 786,\n",
       " '▁i': 787,\n",
       " '▁Frau': 788,\n",
       " '▁provide': 789,\n",
       " 'ma': 790,\n",
       " '▁financial': 791,\n",
       " '▁using': 792,\n",
       " 'un': 793,\n",
       " '▁based': 794,\n",
       " '▁Haus': 795,\n",
       " '▁hand': 796,\n",
       " '▁necessary': 797,\n",
       " '▁insbesondere': 798,\n",
       " '▁full': 799,\n",
       " '▁power': 800,\n",
       " 'ischen': 801,\n",
       " '▁home': 802,\n",
       " 'liche': 803,\n",
       " '▁always': 804,\n",
       " '▁therefore': 805,\n",
       " '▁second': 806,\n",
       " '▁lassen': 807,\n",
       " '▁TV': 808,\n",
       " '▁family': 809,\n",
       " '▁weiß': 810,\n",
       " '▁another': 811,\n",
       " '▁dazu': 812,\n",
       " '▁large': 813,\n",
       " 'Y': 814,\n",
       " '▁Hotels': 815,\n",
       " '▁off': 816,\n",
       " 'as': 817,\n",
       " '▁project': 818,\n",
       " '▁kein': 819,\n",
       " '▁Recht': 820,\n",
       " 'J': 821,\n",
       " 'EN': 822,\n",
       " 'X': 823,\n",
       " '▁2016': 824,\n",
       " '▁sagen': 825,\n",
       " '▁mal': 826,\n",
       " 'v': 827,\n",
       " '▁future': 828,\n",
       " 'ste': 829,\n",
       " '▁Directive': 830,\n",
       " 'bar': 831,\n",
       " '▁real': 832,\n",
       " '▁2.': 833,\n",
       " 'il': 834,\n",
       " '▁next': 835,\n",
       " '▁Personen': 836,\n",
       " '▁Jesus': 837,\n",
       " '▁human': 838,\n",
       " '▁little': 839,\n",
       " '▁look': 840,\n",
       " '▁Anwendung': 841,\n",
       " '▁Ende': 842,\n",
       " '▁offer': 843,\n",
       " '▁Ziel': 844,\n",
       " '▁möglich': 845,\n",
       " '▁mm': 846,\n",
       " '▁group': 847,\n",
       " '▁together': 848,\n",
       " '▁More': 849,\n",
       " '▁kommen': 850,\n",
       " '▁‘': 851,\n",
       " '▁taken': 852,\n",
       " '▁air': 853,\n",
       " '▁Als': 854,\n",
       " '▁Bar': 855,\n",
       " '▁Rates': 856,\n",
       " '▁2018': 857,\n",
       " '▁training': 858,\n",
       " '▁really': 859,\n",
       " '▁account': 860,\n",
       " '▁einige': 861,\n",
       " '▁hotel': 862,\n",
       " '▁2019': 863,\n",
       " '▁China': 864,\n",
       " 'and': 865,\n",
       " '▁change': 866,\n",
       " '▁experience': 867,\n",
       " '▁better': 868,\n",
       " '▁house': 869,\n",
       " '▁access': 870,\n",
       " 'zu': 871,\n",
       " '▁unseren': 872,\n",
       " '▁areas': 873,\n",
       " '▁certain': 874,\n",
       " '▁Our': 875,\n",
       " '▁control': 876,\n",
       " 'no': 877,\n",
       " '▁person': 878,\n",
       " '▁Al': 879,\n",
       " '▁Le': 880,\n",
       " '▁industry': 881,\n",
       " '▁22': 882,\n",
       " '▁II': 883,\n",
       " '▁total': 884,\n",
       " '▁protection': 885,\n",
       " 'ta': 886,\n",
       " '19': 887,\n",
       " '▁Un': 888,\n",
       " '▁website': 889,\n",
       " '▁Unter': 890,\n",
       " '▁question': 891,\n",
       " '▁old': 892,\n",
       " '▁x': 893,\n",
       " '6': 894,\n",
       " '▁list': 895,\n",
       " '▁non': 896,\n",
       " '▁These': 897,\n",
       " '▁International': 898,\n",
       " '▁systems': 899,\n",
       " '▁nichts': 900,\n",
       " '▁given': 901,\n",
       " '▁Ihren': 902,\n",
       " '▁Information': 903,\n",
       " 'ul': 904,\n",
       " '▁page': 905,\n",
       " 'ungen': 906,\n",
       " '▁Grund': 907,\n",
       " '▁light': 908,\n",
       " '▁>': 909,\n",
       " '▁third': 910,\n",
       " 'é': 911,\n",
       " 'ster': 912,\n",
       " '▁2010': 913,\n",
       " '▁weitere': 914,\n",
       " '▁put': 915,\n",
       " '▁gehen': 916,\n",
       " '▁main': 917,\n",
       " '®': 918,\n",
       " '▁something': 919,\n",
       " 'ke': 920,\n",
       " '▁möchte': 921,\n",
       " '▁hin': 922,\n",
       " 'da': 923,\n",
       " '▁results': 924,\n",
       " '8': 925,\n",
       " '▁local': 926,\n",
       " 'ka': 927,\n",
       " '▁activities': 928,\n",
       " '▁wirklich': 929,\n",
       " '▁August': 930,\n",
       " 'EU': 931,\n",
       " '▁Online': 932,\n",
       " '▁Lage': 933,\n",
       " '▁verschiedenen': 934,\n",
       " 'ber': 935,\n",
       " '▁Contact': 936,\n",
       " '▁hast': 937,\n",
       " '▁2014': 938,\n",
       " '▁basis': 939,\n",
       " '▁2013': 940,\n",
       " '▁World': 941,\n",
       " '▁away': 942,\n",
       " '▁until': 943,\n",
       " '▁2012': 944,\n",
       " '▁geben': 945,\n",
       " 'ler': 946,\n",
       " 'os': 947,\n",
       " '▁Dies': 948,\n",
       " '10': 949,\n",
       " '▁today': 950,\n",
       " '▁2009': 951,\n",
       " '▁Group': 952,\n",
       " '▁nun': 953,\n",
       " 'lichen': 954,\n",
       " 'di': 955,\n",
       " '▁few': 956,\n",
       " '▁San': 957,\n",
       " '▁See': 958,\n",
       " '▁Mal': 959,\n",
       " 'ro': 960,\n",
       " '▁things': 961,\n",
       " '▁t': 962,\n",
       " '▁wollen': 963,\n",
       " '▁letzten': 964,\n",
       " '▁Sicherheit': 965,\n",
       " '▁law': 966,\n",
       " '▁site': 967,\n",
       " '▁However': 968,\n",
       " '▁Nicht': 969,\n",
       " '▁worden': 970,\n",
       " '▁cm': 971,\n",
       " '▁innerhalb': 972,\n",
       " '▁Form': 973,\n",
       " '▁Hier': 974,\n",
       " 'j': 975,\n",
       " '▁60': 976,\n",
       " '▁USA': 977,\n",
       " '▁US': 978,\n",
       " '▁less': 979,\n",
       " '▁fact': 980,\n",
       " '▁common': 981,\n",
       " 'am': 982,\n",
       " '▁trade': 983,\n",
       " '▁z': 984,\n",
       " '▁Energie': 985,\n",
       " '▁Search': 986,\n",
       " '▁Fragen': 987,\n",
       " '▁general': 988,\n",
       " 'ra': 989,\n",
       " '▁ja': 990,\n",
       " '▁Mehr': 991,\n",
       " '▁design': 992,\n",
       " '▁interest': 993,\n",
       " '▁23': 994,\n",
       " '▁Weg': 995,\n",
       " '▁Website': 996,\n",
       " '▁present': 997,\n",
       " '▁Europäische': 998,\n",
       " '▁contact': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 33.2MB/s]\n",
      "Downloading data: 100%|██████████| 280M/280M [00:02<00:00, 102MB/s]  \n",
      "Downloading data: 100%|██████████| 265M/265M [00:02<00:00, 92.0MB/s] \n",
      "Downloading data: 100%|██████████| 273M/273M [00:02<00:00, 99.9MB/s] \n",
      "Downloading data: 100%|██████████| 474k/474k [00:00<00:00, 2.43MB/s]\n",
      "Downloading data: 100%|██████████| 509k/509k [00:00<00:00, 2.63MB/s]\n",
      "Generating train split: 100%|██████████| 4508785/4508785 [00:21<00:00, 207200.75 examples/s]\n",
      "Generating validation split: 100%|██████████| 3000/3000 [00:00<00:00, 187555.52 examples/s]\n",
      "Generating test split: 100%|██████████| 3003/3003 [00:00<00:00, 187936.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('wmt14', 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "printed = False\n",
    "def preprocess_function(examples):\n",
    "    global printed\n",
    "    source_lang = 'en'\n",
    "    target_lang = 'de'\n",
    "    inputs = [example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4508785 [00:00<?, ? examples/s]/homes/tl2020/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map:   3%|▎         | 115000/4508785 [00:21<13:44, 5331.40 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 869\u001b[0m     {\n\u001b[1;32m    870\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    871\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    872\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    873\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    874\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    875\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    876\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    877\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    878\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    879\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    880\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    881\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    882\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    883\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    884\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    885\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    886\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    887\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    888\u001b[0m         )\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    890\u001b[0m     }\n\u001b[1;32m    891\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:870\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    869\u001b[0m     {\n\u001b[0;32m--> 870\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    890\u001b[0m     }\n\u001b[1;32m    891\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3163\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3552\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3548\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3550\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3552\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3556\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3561\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3421\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3420\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3421\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3423\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3424\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3425\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[34], line 8\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [example[source_lang] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      7\u001b[0m targets \u001b[38;5;241m=\u001b[39m [example[target_lang] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m----> 8\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mas_target_tokenizer():\n\u001b[1;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m tokenizer(targets, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2945\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2944\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2945\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3032\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3028\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3029\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3030\u001b[0m         )\n\u001b[1;32m   3031\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3034\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3054\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3055\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3228\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3218\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3219\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3220\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3221\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3226\u001b[0m )\n\u001b[0;32m-> 3228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3245\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:873\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m--> 873\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    875\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:841\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    840\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:718\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    716\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m--> 718\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:727\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder:\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder[token]\n\u001b[0;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:182\u001b[0m, in \u001b[0;36mMarianTokenizer._convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;124;03m\"\"\"Cover moses empty string edge case. They return empty list for '' input!\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpunc_normalizer(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_token_to_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_encoder\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_encoder[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_token])\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_language_code\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
